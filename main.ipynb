{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f334725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b17fb8",
   "metadata": {},
   "source": [
    "# Part 1: Data Exploration & Preprocessing\n",
    "Deliverables:\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "- Statistical summary of all features\n",
    "- Class distribution analysis (critical for classification!)\n",
    "    - Bar plot of class counts\n",
    "    - Calculate class imbalance ratio\n",
    "- Distribution plots for key numerical features\n",
    "- Correlation matrix/heatmap\n",
    "- Identification of outliers\n",
    "- Missing value analysis (if applicable)\n",
    "- Categorical feature analysis (unique values, frequency)\n",
    "2. Data Preprocessing Pipeline\n",
    "\n",
    "- Handle missing values\n",
    "- Encode categorical variables (one-hot, label encoding, etc.)\n",
    "- Feature scaling/normalization\n",
    "- Address class imbalance (if applicable):\n",
    "    - SMOTE (Synthetic Minority Over-sampling)\n",
    "    - Undersampling\n",
    "    - Class weights\n",
    "    - Or justify why no balancing is needed\n",
    "- Train/test split with stratification\n",
    "\n",
    "Required Output:\n",
    "- Jupyter notebook or Python script with markdown/comments\n",
    "- Minimum 5 visualizations including class distribution\n",
    "- Written justification for preprocessing choices (3-4 paragraphs)\n",
    "- Discussion of class imbalance strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data into a pandas dataframe\n",
    "df = pd.read_csv(\"./data/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fdf62e",
   "metadata": {},
   "source": [
    "# Part 2: Baseline Model\n",
    "Implement Logistic Regression as your baseline.\n",
    "\n",
    "Report:\n",
    "- Training and test performance metrics (see Part 4)\n",
    "- Confusion Matrix (with visualization)\n",
    "- Classification Report (precision, recall, F1 per class)\n",
    "- ROC Curve and AUC (for binary classification)\n",
    "- Brief analysis of baseline performance\n",
    "    - Where does the model succeed/fail?\n",
    "    - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ffad6",
   "metadata": {},
   "source": [
    "# Part 3: Model Implementation & Comparison\n",
    "Implement at least 4 different classification models. Must include variety:\n",
    "\n",
    "Required Model Types:\n",
    "\n",
    "1. At least one ensemble method (Random Forest, Gradient Boosting, XGBoost, AdaBoost)\n",
    "2. At least one tree-based model (Decision Tree, Extra Trees)\n",
    "3. At least one model from your group research (SVM, KNN, Naive Bayes, Neural Network, etc.)\n",
    "4. Your choice for the fourth\n",
    "\n",
    "For Each Model:\n",
    "1. Initial Training\n",
    "\n",
    "- Train with default hyperparameters\n",
    "- Record training time\n",
    "- Calculate all metrics (see Part 4)\n",
    "\n",
    "2. Document Model Configuration\n",
    "<details>\n",
    "<summary>Example:</summary>\n",
    "model_config = { <br>\n",
    "    &emsp;'name': 'Random Forest', <br>\n",
    "    &emsp;'hyperparameters': { <br>\n",
    "        &emsp;&emsp;'n_estimators': 100, <br>\n",
    "        &emsp;&emsp;'max_depth': None, <br>\n",
    "        &emsp;&emsp;'min_samples_split': 2, <br>\n",
    "        &emsp;&emsp;... <br>\n",
    "    }, <br>\n",
    "    &emsp;'preprocessing_requirements': 'Standard scaling applied', <br>\n",
    "    &emsp;'training_time': 2.34,  # seconds <br>\n",
    "    &emsp;'class_weight': 'balanced'  # if applicable <br>\n",
    "}\n",
    "</details>\n",
    "3. Feature Importance (if applicable)\n",
    "\n",
    "- Plot top 10 most important features\n",
    "- Discuss which features drive predictions\n",
    "- Compare feature importance across models\n",
    "4. Model-Specific Analysis\n",
    "\n",
    "- For tree-based: Visualize decision tree (if feasible)\n",
    "- For SVM: Discuss kernel choice\n",
    "- For KNN: Justify k value selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a3aea3",
   "metadata": {},
   "source": [
    "# Part 4: Model Evaluation\n",
    "Calculate the following metrics for ALL models (baseline + 4):\n",
    "\n",
    "Required Metrics:\n",
    "\n",
    "For Binary Classification:\n",
    "\n",
    "- Accuracy (with discussion of when it's misleading)\n",
    "- Precision (per class and macro/weighted average)\n",
    "- Recall/Sensitivity (per class and macro/weighted average)\n",
    "- F1-Score (per class and macro/weighted average)\n",
    "- AUC-ROC (Area Under ROC Curve)\n",
    "- AUC-PR (Area Under Precision-Recall Curve - important for imbalanced data)\n",
    "- Confusion Matrix\n",
    "\n",
    "For Multiclass Classification:\n",
    "\n",
    "- All of the above (adapted for multiclass)\n",
    "- Macro-average and Weighted-average metrics\n",
    "- Per-class performance analysis\n",
    "\n",
    "Create Comparison Visualizations:\n",
    "\n",
    "1. Metrics Comparison Table\n",
    "\n",
    "| Model            | Accuracy | Precision | Recall | F1    | AUC-ROC | Train Time |\n",
    "|------------------|----------|-----------|--------|-------|---------|------------|\n",
    "| Logistic Reg     | 0.92     | 0.85      | 0.78   | 0.81  | 0.94    | 0.12s      |\n",
    "| Random Forest    | 0.94     | 0.88      | 0.82   | 0.85  | 0.96    | 2.45s      |\n",
    "| ...              | ...      | ...       | ...    | ...   | ...     | ...        |\n",
    "\n",
    "2. Bar Charts: Compare metrics across models\n",
    "\n",
    "3. Confusion Matrices: For ALL models (use subplots)\n",
    "\n",
    "4. ROC Curves: Plot all models on same graph for comparison\n",
    "\n",
    "5. Precision-Recall Curves: Especially important for imbalanced datasets\n",
    "\n",
    "6. Learning Curves (Optional but recommended): For your top 2 models\n",
    "\n",
    "- Plot training vs validation score as function of training size\n",
    "- Helps diagnose overfitting/underfitting\n",
    "\n",
    "Analysis Requirements:\n",
    "- Which metric is most important for your problem? Why?\n",
    "- Discuss the precision-recall tradeoff\n",
    "- For imbalanced data: Why is accuracy potentially misleading?\n",
    "- Compare performance on minority vs. majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba6c2f",
   "metadata": {},
   "source": [
    "# Part 5: Hyperparameter Tuning\n",
    "Select your best performing model from Part 3 and optimize it.\n",
    "\n",
    "Required Approach:\n",
    "Use GridSearchCV or RandomizedSearchCV\n",
    "<details>\n",
    "<summary>Implementation:</summary>\n",
    "from sklearn.model_selection import GridSearchCV <br>\n",
    "<br>\n",
    "param_grid = { <br>\n",
    "&emsp;    # Define at least 3 hyperparameters to tune <br>\n",
    "&emsp;    # Each with at least 3 different values <br>\n",
    "&emsp;    # Example for Random Forest: <br>\n",
    "&emsp;    'n_estimators': [100, 200, 300], <br>\n",
    "&emsp;    'max_depth': [10, 20, 30, None], <br>\n",
    "&emsp;    'min_samples_split': [2, 5, 10], <br>\n",
    "&emsp;    'class_weight': ['balanced', None]  # if imbalanced <br>\n",
    "} <br>\n",
    "<br>\n",
    "grid_search = GridSearchCV( <br>\n",
    "&emsp;    estimator=your_model, <br>\n",
    "&emsp;    param_grid=param_grid, <br>\n",
    "&emsp;    cv=5,  # 5-fold stratified cross-validation <br>\n",
    "&emsp;    scoring='f1',  # or 'roc_auc', 'f1_weighted', etc. <br>\n",
    "&emsp;    n_jobs=-1, <br>\n",
    "&emsp;    verbose=1 <br>\n",
    ") <br>\n",
    "<br>\n",
    "grid_search.fit(X_train, y_train)\n",
    "</details>\n",
    "\n",
    "Document:\n",
    "- Initial hyperparameters vs. optimal hyperparameters\n",
    "- Performance improvement (before/after tuning)\n",
    "    - Show metrics table comparing both versions\n",
    "- Cross-validation scores (mean and std)\n",
    "- Training time comparison\n",
    "- Scoring metric choice: Why did you choose that metric for optimization?\n",
    "- Discussion: Was the tuning worth the computational cost?\n",
    "Additional Consideration:\n",
    "- If using imbalanced data, ensure CV is stratified\n",
    "- Consider using multiple scoring metrics\n",
    "- Discuss any tradeoffs (e.g., precision vs. recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee804fa6",
   "metadata": {},
   "source": [
    "Part 6: Reflections\n",
    "Write a comprehensive report addressing:\n",
    "\n",
    "1. Model Selection Justification\n",
    "- Why did certain models perform better than others?\n",
    "- What characteristics of the data favor specific algorithms?\n",
    "- Were there any surprising results?\n",
    "- How did class imbalance (if present) affect different models?\n",
    "2. Feature Analysis\n",
    "- Which features were most important across models?\n",
    "- Did feature importance differ between models?\n",
    "- Any features that were unexpectedly important/unimportant?\n",
    "- Recommendations for feature engineering\n",
    "3. Practical Considerations\n",
    "- Which model would you deploy in production and why?\n",
    "    - Consider: accuracy, speed, interpretability, maintenance\n",
    "- Trade-offs:\n",
    "    - Accuracy vs. interpretability vs. speed\n",
    "    - Precision vs. recall (what's more important for your use case?)\n",
    "- Ethical considerations (especially for sensitive domains):\n",
    "    - Potential biases in the model\n",
    "    - Fairness across different groups\n",
    "    - Consequences of false positives vs. false negatives\n",
    "- Potential limitations of your models\n",
    "4. Future Improvements\n",
    "- What would you try next to improve performance?\n",
    "- Additional data that would be helpful\n",
    "- Different approaches to consider:\n",
    "    - Ensemble methods (stacking, voting)\n",
    "    - Deep learning\n",
    "    - Different feature engineering\n",
    "    - Cost-sensitive learning\n",
    "- How would you monitor model performance in production?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
